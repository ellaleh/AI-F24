For my final project, I developed two machine learning algorithms that conduct sentiment analysis on a Kaggle dataset of IMDB movie reviews. The algorithms classify reviews as positive or negative based on the text content, demonstrating Natural Language Processing (NLP) techniques in binary sentiment classification. 

Sentiment analysis is a fundamental task in Natural Language Processing that includes a “series of methods, techniques, and tools about detecting and extracting subjective information, such as opinion and attitudes, from language” (Mäntylä). Applications for sentiment analysis include customer feedback analysis, social media monitoring, and recommendation systems. Historically, sentiment analysis began with lexicon-based approaches, where predefined dictionaries of positive and negative words were used to classify sentiment. However, these methods often struggled with ambiguity, contextual nuances, and evolving language usage. Some machine learning approaches to sentiment analysis, such as logistic regression or support vector machines (SVM), rely on features like Term Frequency Inverse Document Frequency (TF-IDF)-- the calculation of how relevant a word is to a text– or bag-of-words– a language model based on word count (“Understanding TF-IDF”) (Bag-of-Words). While effective for small datasets, these models struggle to capture long-term dependencies in text. Similarly, deep learning through Recurrent Neural Networks (RNNs) can handle sequential data, but RNNs suffer from vanishing gradient issues which limit their capacity to learn long-term dependencies. 

Long Short-Term Memory (LSTM) networks, “a type of recurrent neural network (RNN) that can learn long-term dependencies between time steps of sequence data”, address these limitations by introducing gated mechanisms that retain relevant information over extended sequences (“Long Short-Term”). Moreover, transformer-based models such as Bidirectional Encoder Representations from Transformers (BERT) leverage self-attention mechanisms, allowing the model to “focus on different parts of the input sequence when processing each element” (Verma). Unlike LSTMs, transformers can process entire sequences in parallel, enabling them to model relationships between words regardless of their distance in the text. Pre-trained models like BERT offer additional advantages because they have been trained on massive bodies of data, such as Wikipedia and BookCorpus, which further helps them capture context. Despite these advantages, models like LSTM and BERT require significant computational resources and are prone to overfitting. Additionally, while transformers excel in understanding context, they are computationally expensive and can be impractical for real-time applications without access to GPUs or TPUs.

My project builds on these advancements, comparing LSTM and BERT architectures for sentiment analysis. The IMDB dataset, which has a balanced class distribution and fifty thousand movie reviews, provided a strong base of data to allow these algorithms the best chance at success. Additionally, I love reading positive and negative reviews for movies after watching them, so I knew I would enjoy the experience of building and testing these ML algorithms.

After testing out some baseline approaches based on codes we had discussed in class– in particular, our in-class code for an RNN sentiment analysis– I implemented an algorithm using LSTM networks. My algorithm initially struggled to accurately sort reviews between positive and negative, and the runtime for my code was several hours long. After refining the algorithm to a point where I was satisfied with its performance, I worked on approaching this problem using the pre-trained embedding BERT. My goal was to learn about an additional way to approach the same problem and compare results.
